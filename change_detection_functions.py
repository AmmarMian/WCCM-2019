##############################################################################
# Functions used to detect a change in the parameters of a SIRV distribution
# Authored by Ammar Mian, 28/09/2018
# e-mail: ammar.mian@centralesupelec.fr
##############################################################################
# Copyright 2018 @CentraleSupelec
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##############################################################################
import numpy as np
import scipy as sp
import scipy.stats
import scipy.special
import warnings
from generic_functions import *




##############################################################################
# Gaussian Statistics
##############################################################################
def Natural_Geodesic_statistic(X, Args):
    """ Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = unused
        Outputs:
            * the statistic given the observations in input"""

    (p, N, T) = .shape


    位 = 0
    for t_1 in range(T):
        _t_1 = SCM([:, :, t_1])
        i_t_1_sqm = np.linalg.inv(sp.linalg.sqrtm(_t_1))
        for t_2 in range(T):
            if t_2 != t_1:
                _t_2 = SCM([:, :, t_2])
                位 = 位 + np.linalg.norm( sp.linalg.logm( i_t_1_sqm @ _t_2 @ i_t_1_sqm ), 'fro')

    return np.real(位)


def Wasserstein_Gaussian_statistic(X, Args):
    """ Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = Unused
        Outputs:
            * the statistic given the observations in input"""

    (p, N, T) = .shape

    位 = 0
    for t_1 in range(T):
        _t_1 = SCM([:, :, t_1])
        _t_1_sqm = sp.linalg.sqrtm(_t_1)
        for t_2 in range(T):
            if t_2 != t_1:
                _t_2 = SCM([:, :, t_2])
                位 = 位 + np.trace(_t_1) + np.trace(_t_2) - 2*np.trace( sp.linalg.sqrtm(_t_1_sqm@_t_2@_t_1_sqm) )

    return np.real(位)


def Kullback_Leibler_divergence(A, B):
    """ Inputs:
            * A, B = covariance matrices
        Outputs:
            * the divergence"""

    p = A.shape[0]
    位 = np.trace(np.linalg.inv(B)@A) + np.log(np.abs(np.linalg.det(B))) - np.log(np.abs(np.linalg.det(A))) - p
    return np.real(位)


def Kullback_Leibler_statistic(, args=None):
    """ Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = Unused
        Outputs:
            * the Kullback-Leibler statistic"""

    (p, N, T) = .shape
    位 = 0
    for t_1 in range(T):
        _t_1 = SCM([:, :, t_1])
        for t_2 in range(T):
            if t_2 > t_1:
                _t_2 = SCM([:, :, t_2])
                位 = 位 +  0.5*(Kullback_Leibler_divergence(_t_1,_t_2) + Kullback_Leibler_divergence(_t_2,_t_1))
    return np.real(位)


def complex_Hotelling_Lawley_trace_statistic(, args=None):
    """ V. Akbari, S. N. Anfinsen, A. P. Doulgeris, T. Eltoft, G. Moser and S. B. Serpico, 
        "Polarimetric SAR Change Detection With the Complex HotellingLawley Trace Statistic," in IEEE Transactions on Geoscience and Remote Sensing, vol. 54, no. 7, pp. 3953-3966, July 2016.
        doi: 10.1109/TGRS.2016.2532320
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = Unused
        Outputs:
            * the CHLT statistic given the observations in input"""

    (p, N, T) = .shape
    位 = 0
    for t_1 in range(T):
        _t_1 = SCM([:, :, t_1])
        i_t_1 = np.linalg.inv(_t_1)
        for t_2 in range(T):
            if t_2 != t_1:
                _t_2 = SCM([:, :, t_2])
                位 = 位 + np.trace(i_t_1@_t_2)
    return np.real(位)


def covariance_equality_glrt_gaussian_statistic(, args=None):
    """ GLRT statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = None
        Outputs:
            * the GLRT statistic given the observations in input"""

    (p, N, T) = .shape
    S = 0
    logDenominator = 0
    for t in range(0, T):
        St = SCM([:, :, t])
        logDenominator = logDenominator + N * np.log(np.abs(np.linalg.det(St)))
        S = S + St / T
    logNumerator = N * T * np.log(np.abs(np.linalg.det(S)))
    if args is not None:
        if args=='log':
            return np.real(logNumerator - logDenominator)
    return np.exp(np.real(logNumerator - logDenominator))


def covariance_equality_t1_gaussian_statistic(, args=None):
    """ t1 statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
        Outputs:
            * the t1 statistic given the observations in input"""

    (N, K, M) = .shape

    Sigma_10 = SCM(.reshape((N, K*M)))
    iSigma_10 = np.linalg.inv(Sigma_10)
    t1 = 0
    for t in range(0, M):
        Sigma_m1 = SCM([:, :, t])
        S = (iSigma_10 @ Sigma_m1)
        t1 = t1 + np.trace( S @ S )/M;

    if args is not None:
        if args=='log':
            return np.log(np.real(t1))
    return np.real(t1)


def covariance_equality_Wald_gaussian_statistic(, args=None):
    """ Wald statistic for detecting a change of covariance matrix in a multivariate Gaussian Time Series.
        At each time, Ni.i.d samples are available. A description of the statistic can be found in:
        D. Ciuonzo, V. Carotenuto and A. De Maio, 
        "On Multiple Covariance Equality Testing with Application to SAR Change Detection," 
        in IEEE Transactions on Signal Processing, vol. 65, no. 19, pp. 5078-5091, 1 Oct.1, 2017.
        doi: 10.1109/TSP.2017.2712124
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
        Outputs:
            * the Wald statistic given the observations in input"""

    (N, K, M) = .shape
    L = 0
    O = 0
    Q = 0
    Sigma_11 = SCM([:, :, 0])
    for m in range(0,M):
        Sigma_m1 = SCM([:, :, m])
        iSigma_m1 = np.linalg.inv(Sigma_m1)
        if m != 0:
            S = np.eye(N) - Sigma_11@iSigma_m1
            L = L + K*np.trace(S@S)
            Q = Q + K*(iSigma_m1 - iSigma_m1@Sigma_11@iSigma_m1)
        O = O + K*np.kron(iSigma_m1.T, iSigma_m1)
    
    if args is not None:
        if args=='log':
            return np.log(np.real(L - vec(Q).conj().T @ (np.linalg.inv(O)@vec(Q))))
    return np.real(L - vec(Q).conj().T @ (np.linalg.inv(O)@vec(Q)))


##############################################################################
# Robust Statistics
##############################################################################

def MLL(X, tol, iter_max):
    """ Case L=1 """
    p, N = X.shape

    (, 未, iteration) = tyler_estimator_covariance(X, tol, iter_max)
     = np.abs(1/p * np.diagonal(.conj().T@np.linalg.inv()@))

    # Estimate Gamma parameters on 
    谓, loc, 渭 = sp.stats.gamma.fit(, floc=0)

    temp = 0
    for i in range(N):
        temp = temp + np.log(sp.special.kv(谓-p, 2*np.sqrt([i]*谓/渭)))



    result = N * (谓 + p)/2 * (np.log(谓) - np.log(渭)) + \
            (谓-p)/2 * p * np.sum() + temp - N*np.log(np.abs(np.linalg.det())) - \
            N * np.log(sp.special.gamma(谓))
    return result
     


def Similarity_measure_Meng_Liu(X, Args):
    """ From M. Liu, H. Zhang, C. Wang and F. Wu, 
    "Change Detection of Multilook Polarimetric SAR Images Using Heterogeneous Clutter Models," 
    in IEEE Transactions on Geoscience and Remote Sensing, vol. 52, no. 12, pp. 7483-7494, Dec. 2014.
    doi: 10.1109/TGRS.2014.2310451
        Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = tol, iterMax for Tyler
        Outputs:
            * the statistic given the observations in input"""

    p, N, T = X.shape
    tol, iter_max = Args

    位 = 0
    for t_1 in range(T):
        for t_2 in range(T):
            if t_2 > t_1:
                位 = 位 + MLL(X[:,:,t_1], tol, iter_max) + MLL(X[:,:,t_2], tol, iter_max) - \
                        MLL(np.hstack([X[:,:,t_1],X[:,:,t_2]]), tol, iter_max)

    return np.real(位)



def Natural_student_t_distance(X, Args):
    """ Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = d=degrees of freedom and tol, iterMax for MLE
        Outputs:
            * the statistic given the observations in input"""

    d, tol, iter_max = Args
    (p, N, T) = .shape

    alpha = (d+p)/(d+p+1)
    beta = alpha - 1

    位 = 0
    for t_1 in range(T):
        (_t_1, 未, niter) = student_t_estimator_covariance_mle([:,:,t_1], d, tol, iter_max)
        for t_2 in range(T):
            if t_2 != t_1:
                (_t_2, 未, niter) = student_t_estimator_covariance_mle([:,:,t_2], d, tol, iter_max)
                w = sp.linalg.eig(_t_1, _t_2, right=False)
                位 = 位 + alpha*np.sum(np.log(w)**2) + beta*np.sum(np.log(w))**2

    return np.real(位)




def Wasserstein_robust_statistic(X, Args):
    """ Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = tol, iterMax for Tyler
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max = Args
    (p, N, T) = .shape


    位 = 0
    for t_1 in range(T):
        (_t_1, 未, iteration) = tyler_estimator_covariance([:,:,t_1], tol, iter_max)
        _t_1 = (1/p) * np.diagonal([:,:,t_1].conj().T@np.linalg.inv(_t_1)@[:,:,t_1])
        _t_1 = np.mean(_t_1)*_t_1
        sqrtm__t_1 = sp.linalg.sqrtm(_t_1)
        for t_2 in range(T):
            if t_2 != t_1:
                
                (_t_2, 未, iteration) = tyler_estimator_covariance([:,:,t_2], tol, iter_max)                
                _t_2 = (1/p) * np.diagonal([:,:,t_2].conj().T@np.linalg.inv(_t_2)@[:,:,t_2])
                _t_2 = np.mean(_t_2)*_t_2

                
                位 = 位 + np.trace(_t_1) + np.trace(_t_2) - 2*np.trace( sp.linalg.sqrtm(sqrtm__t_1@_t_2@sqrtm__t_1) )

    return np.real(位)



def student_t_glrt_statistic_d_known(X, Args):
    """ GLRT test for testing a change in the covariance of a multivariate
        Student-t distribution when the degree of freefom is known.
        Inputs:
            * X = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * Args = d=degree of freedom and tol, iterMax for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    d, tol, iter_max, scale = Args
    (p, N, T) = .shape

    # Estimating _0 using all the observations
    (_0, 未, niter) = student_t_estimator_covariance_mle(.reshape((p,T*N)), d, tol, iter_max)
    i_0 = np.linalg.inv(_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(_0)))
    log_denominator_determinant_terms = 0
    log_0 = 0
    log_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating _t
        (_t, 未, iteration) = student_t_estimator_covariance_mle([:,:,t], d, tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(_t)))

        # Computing quadratic terms
        log_0 =  log_0 + np.log(d + np.diagonal([:,:,t].conj().T@i_0@[:,:,t]))
        log_t = log_t + np.log(d + np.diagonal([:,:,t].conj().T@np.linalg.inv(_t)@[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = (d+p)*np.sum(log_0)
    log_denominator_quadtratic_terms = (d+p)*np.sum(log_t)

    # Final expression of the statistic
    if scale=='linear':
        位 = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        位 = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return 位


def scale_and_shape_equality_robust_statistic(, args):
    """ GLRT test for testing a change in the scale or/and shape of 
        a deterministic SIRV model.
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = .shape

    # Estimating _0 using all the observations
    (_0, 未, niter) = tyler_estimator_covariance_matandtext(, tol, iter_max)
    i_0 = np.linalg.inv(_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(_0)))
    log_denominator_determinant_terms = 0
    _0 = 0
    log_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating _t
        (_t, 未, iteration) = tyler_estimator_covariance([:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(_t)))

        # Computing texture estimation
        _0 =  _0 + np.diagonal([:,:,t].conj().T@i_0@[:,:,t]) / T
        log_t = log_t + np.log(np.diagonal([:,:,t].conj().T@np.linalg.inv(_t)@[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(_0))
    log_denominator_quadtratic_terms = p*np.sum(log_t)

    # Final expression of the statistic
    if scale=='linear':
        位 = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        位 = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return 位


def shape_equality_robust_statistic(, args):
    """ GLRT test for testing a change in the shape of 
        a deterministic SIRV model.
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = .shape

    # Estimating _0 using all the observations
    (_0, 未, niter) = tyler_estimator_covariance(.reshape((p,T*N)), tol, iter_max)
    i_0 = np.linalg.inv(_0)

    # Some initialisation
    log_numerator_determinant_terms = T*N*np.log(np.abs(np.linalg.det(_0)))
    log_denominator_determinant_terms = 0
    log_0 = 0
    log_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):
        # Estimating _t
        (_t, 未, iteration) = tyler_estimator_covariance([:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(_t)))

        # Computing texture estimation
        log_0 =  log_0 + np.log(np.diagonal([:,:,t].conj().T@i_0@[:,:,t]))
        log_t = log_t + np.log(np.diagonal([:,:,t].conj().T@np.linalg.inv(_t)@[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = p*np.sum(log_0)
    log_denominator_quadtratic_terms = p*np.sum(log_t)

    # Final expression of the statistic
    if scale=='linear':
        位 = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        位 = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return 位


def scale_equality_robust_statistic(, args):
    """ GLRT test for testing a change in the scale of 
        a deterministic SIRV model.
        Inputs:
            *  = a (p, N, T) numpy array with:
                * p = dimension of vectors
                * N = number of Samples at each date
                * T = length of time series
            * args = tol, iter_max for Tyler, scale
        Outputs:
            * the statistic given the observations in input"""

    tol, iter_max, scale = args
    (p, N, T) = .shape

    # Estimating _t under H0 regime using all the observations
    (_0, , niter) = tyler_estimator_covariance_text(, tol, iter_max)

    # Some initialisation
    log_numerator_determinant_terms = 0
    log_denominator_determinant_terms = 0
    _0 = 0
    log_t = 0
    # Iterating on each date to compute the needed terms
    for t in range(0,T):

        # Estimating _t under H1 regime
        (_t, 未, iteration) = tyler_estimator_covariance([:,:,t], tol, iter_max)

        # Computing determinant add adding it to log_numerator_determinant_terms
        log_numerator_determinant_terms = log_numerator_determinant_terms + \
                                        N*np.log(np.abs(np.linalg.det(_0[:,:,t])))

        # Computing determinant add adding it to log_denominator_determinant_terms
        log_denominator_determinant_terms = log_denominator_determinant_terms + \
                                            N*np.log(np.abs(np.linalg.det(_t)))

        # Computing texture estimation
        _0 =  _0 + np.diagonal([:,:,t].conj().T@np.linalg.inv(_0[:,:,t])@[:,:,t]) / T
        log_t = log_t + np.log(np.diagonal([:,:,t].conj().T@np.linalg.inv(_t)@[:,:,t]))

    # Computing quadratic terms
    log_numerator_quadtratic_terms = T*p*np.sum(np.log(_0))
    log_denominator_quadtratic_terms = p*np.sum(log_t)

    # Final expression of the statistic
    if scale=='linear':
        位 = np.exp(np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms))
    else:
        位 = np.real(log_numerator_determinant_terms - log_denominator_determinant_terms + \
        log_numerator_quadtratic_terms - log_denominator_quadtratic_terms)

    return 位

##############################################################################
# Some Functions
##############################################################################
def tyler_estimator_covariance_matandtext(, tol=0.0001, iter_max=20):
    """ A function that computes the Modified Tyler Fixed Point Estimator for 
    covariance matrix estimation under problem MatAndText.
        Inputs:
            *  = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            *  = the estimate
            * 未 = the final distance between two iterations
            * iteration = number of iterations til convergence """

    (p, N, T) = .shape
    未 = np.inf # Distance between two iterations
     = np.eye(p) # Initialise estimate to identity
    iteration = 0

    # Recursive algorithm
    while (未>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates avalaibe
         = 0
        i = np.linalg.inv()
        for t in range(0, T):
             =  + np.diagonal([:,:,t].conj().T@i@[:,:,t])

        # Computing expression of the estimator
        _new = 0
        for t in range(0, T):
            _bis = [:,:,t] / np.sqrt()
            _new = _new + (p/N) * _bis@_bis.conj().T

        # Imposing trace constraint: Tr() = p
        _new = p*_new/np.trace(_new)

        # Condition for stopping
        未 = np.linalg.norm(_new - , 'fro') / np.linalg.norm(, 'fro')

        # Updating 
         = _new
        iteration = iteration + 1

    if iteration == iter_max:
        warnings.warn('Recursive algorithm did not converge')

    return (, 未, iteration)


def tyler_estimator_covariance_text(, tol=0.0001, iter_max=20):
    """ A function that computes the Modified Tyler Fixed Point Estimator for
    covariance matrix estimation under problem TextGen.
        Inputs:
            *  = a matrix of size p*N*T with each saptial observation along column dimension and time
                observation along third dimension.
            * tol = tolerance for convergence of estimator
            * iter_max = number of maximum iterations
        Outputs:
            *  = array of size (p,p,T) to the different estimates
            *  = the final distance between two iterations for each estimate
            * iteration = number of iterations til convergence """

    (p, N, T) = .shape
     = np.inf*np.ones(T) # Distance between two iterations for each t
     = np.tile(np.eye(p).reshape(p,p,1), (1,1,T)) # Initialise all estimates to identity
    iteration = 0

    # Recursive algorithm
    while (np.max()>tol) and iteration < iter_max:

        # Compute the textures for each pixel using all the dates avalaibe
         = 0
        for t in range(0, T):
            i_t = np.linalg.inv([:,:,t])
             =  + np.diagonal([:,:,t].conj().T@i_t@[:,:,t])

        # Computing expression of the estimator
        _new = np.zeros((p,p,T)).astype(complex)
        for t in range(0, T):
            _bis = [:,:,t] / np.sqrt()
            _new[:,:,t] = (T*p/N) * _bis@_bis.conj().T

            # Imposing trace constraint: Tr() = p
            _new[:,:,t] = p*_new[:,:,t]/np.trace(_new[:,:,t])

            # Condition for stopping
            [t] = np.linalg.norm(_new[:,:,t] - [:,:,t], 'fro') / \
                     np.linalg.norm([:,:,t], 'fro')
        
        # Updating 
         = _new
        iteration = iteration + 1

    if iteration == iter_max:
        warnings.warn('Recursive algorithm did not converge')

    return (, , iteration)